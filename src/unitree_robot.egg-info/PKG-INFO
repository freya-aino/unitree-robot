Metadata-Version: 2.4
Name: unitree-robot
Version: 0.1.0
Summary: Add your description here
Requires-Python: >=3.13
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.9.0
Requires-Dist: jax[cpu]; sys_platform == "win32"
Requires-Dist: jax[cuda13]; sys_platform == "linux"
Requires-Dist: mlflow>=3.5.1
Requires-Dist: warp-lang>=1.10.0
Requires-Dist: mujoco-mjx>=3.3.7
Requires-Dist: tqdm>=4.67.1
Requires-Dist: hydra-core>=1.3.2
Requires-Dist: flatten-dict>=0.4.2
Requires-Dist: nvidia-ml-py>=13.580.82
Requires-Dist: torchrl>=0.10.1

# Description

# Goal

# Process

### Sensor Processing

- [ ] UnitreeSdk2Python connector for collected async data extraction
    - [ ] Lidar Point Cloud
    - [ ] IMU
    - [ ] Global position (#todo: verify calibration / initialization)
    - [ ] Actuator State
    - [ ] Foot Preasuer (go2)
    - [ ] (#todo: other)
- [ ] Move to `state_real(t_real)`
- [ ] External connector
    - [ ] (Depth Camera)
    - [ ] Camera
- [ ] Add Message queue

### Data Control Association

- [ ] `state_real(t_real) <-> [usd/usrd/mjcf] state_sim(t_sim)` semantic mapping (#todo: try using LLM for general purpose static transcription generation).
- [ ] #todo: what happens to the initial pose in 3D space, can we expect to reset sim or real state position.

### Pose Optimization (Constraint Enforcement & sim2sim)

- [ ] `f(state_real[t]) -> state_sim[t]` (observe)
- [ ] `c(state_sim[t], priors, constraints) -> state_sim[t+1]` (simulate, constraint)
- [ ] `p(state_sim[t+1]) -> state_real[t+1]*` (action)
- [ ] apply `state_real[t+1]*`
- [ ] `g(state_real[t+1], state_real[t+1]*)` (feedback)


- [ ] `f(state_real, t_real, state_sim*) -> d_state_sim*/d_t_sim`
- [ ] `constraint_fn(d_state_sim*/d_t_sim) -> d_state_sim/d_t_sim`
- [ ] `simulation_fn(state_sim, d_state_sim/d_t_sim) -> state_sim(t_sim+1)`
- [ ] `c(d_state_real*/d_t_real, state_real, constraint_model, ) -> d_state_real/d_t_real`
