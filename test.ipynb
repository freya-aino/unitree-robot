{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8852cd1e-e95b-42d4-b788-dd8e2731027f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with initialize(version_base=None, config_path=\"conf\"):\n",
    "    cfg = compose(config_name=\"config\", overrides=[\"device=cpu\"])\n",
    "    OmegaConf.resolve(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45267e40-5104-4ca8-8441-0a8582948d0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:37:14.307381Z",
     "start_time": "2025-11-12T13:37:14.303323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': {'lr': 0.008}, 'lr_scheduler': {'linear_lr_warmup': {'start_factor': 0.25, 'end_factor': 1.0, 'total_iters': 20}, 'cosine_annealing_warm_restarts': {'T_0': 40, 'T_mult': 1, 'eta_min': 1e-05}}, 'train_epochs': 500, 'num_parallel_environments': 200, 'device': 'cpu', 'random_seed': 0, 'experiment_name': 'standing_up', 'xla_gpu_memory_fraction': 0.6, 'environment': {'num_parallel_environments': 200, 'sim_frames_per_step': 5, 'mujoco_timestep': 0.005, 'model_path': './external/unitree_mj_models/go2/scene.xml', 'initial_noise_scale': 0.1, 'observation_size': 37, 'action_size': 12}, 'experiment': {'body_name': 'base_link', 'body_angle_reward_scale': 1.0, 'body_height_reward_scale': 1.0, 'energy_reward_scale': 1.0, 'distance_from_origin_reward_scale': 1.0, 'joint_limit_reward_scale': 1.0}, 'agent': {'observation_size': 37, 'action_size': 12, 'train_sequence_length': 200, 'network_hidden_size': 16, 'num_hidden_layers': 0, 'lambda_': 0.99, 'epsilon': 0.3, 'discounting': 0.97, 'reward_scaling': 0.1, 'moving_average_window_size': 16, 'policy_loss_scale': 1, 'value_loss_scale': 0.5, 'entropy_loss_scale': 0.01}, 'training': {'train_epochs': 500, 'unroll_length': 200, 'minibatch_size': None, 'max_gradient_norm': 5.0}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29bf918a-d4d2-445a-99ac-192096d4c3e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:37:20.203284Z",
     "start_time": "2025-11-12T13:37:16.193370Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'qpos',\n",
       " 'qvel',\n",
       " 'act',\n",
       " 'qacc_warmstart',\n",
       " 'plugin_state',\n",
       " 'ctrl',\n",
       " 'qfrc_applied',\n",
       " 'xfrc_applied',\n",
       " 'eq_active',\n",
       " 'mocap_pos',\n",
       " 'mocap_quat',\n",
       " 'qacc',\n",
       " 'act_dot',\n",
       " 'userdata',\n",
       " 'sensordata',\n",
       " 'xpos',\n",
       " 'xquat',\n",
       " 'xmat',\n",
       " 'xipos',\n",
       " 'ximat',\n",
       " 'xanchor',\n",
       " 'xaxis',\n",
       " 'ten_length',\n",
       " 'geom_xpos',\n",
       " 'geom_xmat',\n",
       " 'site_xpos',\n",
       " 'site_xmat',\n",
       " 'cam_xpos',\n",
       " 'cam_xmat',\n",
       " 'subtree_com',\n",
       " 'cvel',\n",
       " 'qfrc_bias',\n",
       " 'qfrc_gravcomp',\n",
       " 'qfrc_fluid',\n",
       " 'qfrc_passive',\n",
       " 'qfrc_actuator',\n",
       " 'actuator_force',\n",
       " 'qfrc_smooth',\n",
       " 'qacc_smooth',\n",
       " 'qfrc_constraint',\n",
       " 'qfrc_inverse',\n",
       " '_impl',\n",
       " '__module__',\n",
       " '__firstlineno__',\n",
       " '__annotations__',\n",
       " '__doc__',\n",
       " 'impl',\n",
       " '__getattr__',\n",
       " '__getitem__',\n",
       " '__static_attributes__',\n",
       " '__dataclass_params__',\n",
       " '__dataclass_fields__',\n",
       " '__replace__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__repr__',\n",
       " '__eq__',\n",
       " '__setattr__',\n",
       " '__delattr__',\n",
       " '__match_args__',\n",
       " 'replace',\n",
       " 'bind',\n",
       " '__init_subclass__',\n",
       " 'fields',\n",
       " 'tree_replace',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '__new__',\n",
       " '__str__',\n",
       " '__getattribute__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__reduce_ex__',\n",
       " '__reduce__',\n",
       " '__getstate__',\n",
       " '__subclasshook__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unitree_robot.common.environments import MujocoMjxEnv\n",
    "\n",
    "environment = MujocoMjxEnv(**cfg.environment)\n",
    "environment.mjx_data_initial.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "53558e3a-72ce-4640-aee2-680df4fd7e57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:44:30.546993Z",
     "start_time": "2025-11-12T13:44:30.515140Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from unitree_robot.common.experiments import Experiment\n",
    "from mujoco.mjx import Data as MjxData\n",
    "from mujoco.mjx import Model as MjxModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bcccb817-7742-469e-997c-0efa832f9dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0., dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# def food_contact(data: MjxData):\n",
    "#     return data.\n",
    "\n",
    "# energy_reward(environment.mjx_data_initial)\n",
    "# environment.mjx_data_initial.xpos\n",
    "\n",
    "experiment = Go2WalkingExperiment(environment.mjx_model, torso_name=\"base_link\", energy_reward_scale=0.1, torso_height_reward_scale=1.0, torso_distance_from_origin_reward_scale=1.0)\n",
    "\n",
    "# mapper.parse_mjx_data(environment.mjx_data_initial)\n",
    "experiment.calculate_reward(environment.mjx_data_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5809bb56e676b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command\t1.0 – 2.0\n",
    "# Energy\t0.01 – 0.1\n",
    "# Contact\t0.2 – 0.6 (per foot)\n",
    "# Height\t0.5 – 1.0\n",
    "# Orientation\t0.01 – 0.05\n",
    "# Smoothness\t0.01 – 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413def30-0e26-47f0-a0a7-595a45729d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from unitree_robot.common.datastructure import UnrollData\n",
    "from unitree_robot.common.agents import PPOAgentTorcRL\n",
    "\n",
    "\n",
    "ud = UnrollData(\n",
    "    num_unrolls = 128,\n",
    "    unroll_length = 500,\n",
    "    observation_size = 32,\n",
    "    action_size = 8\n",
    ")\n",
    "\n",
    "pg_net = PPOAgentTorcRL(\n",
    "    observation_size=32,\n",
    "    action_size=8,\n",
    "    network_hidden_size=64,\n",
    "    discounting=0.99,\n",
    "    lambda_=0.97,\n",
    "    epsilon=0.25,\n",
    "    policy_loss_scale=1.0,\n",
    "    value_loss_scale=0.5,\n",
    "    entropy_loss_scale=0.01,\n",
    "\n",
    "    \n",
    "    num_hidden_layers=1,\n",
    "    moving_average_window_size=10,\n",
    "    reward_scaling=1.0,\n",
    "    train_sequence_length=10\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # we now have a batch of data to work with. Let's learn something from it.\n",
    "\n",
    "# # We'll need an \"advantage\" signal to make PPO work.\n",
    "# # We re-compute it at each epoch as its value depends on the value\n",
    "# # network which is updated in the inner loop.\n",
    "# advantage_module(tensordict_data)\n",
    "# data_view = tensordict_data.reshape(-1)\n",
    "# replay_buffer.extend(data_view.cpu())\n",
    "# for _ in range(frames_per_batch // sub_batch_size):\n",
    "#     subdata = replay_buffer.sample(sub_batch_size)\n",
    "#     loss_vals = loss_module(subdata.to(device))\n",
    "#     loss_value = (\n",
    "#         loss_vals[\"loss_objective\"]\n",
    "#         + loss_vals[\"loss_critic\"]\n",
    "#         + loss_vals[\"loss_entropy\"]\n",
    "#     )\n",
    "\n",
    "#     # Optimization: backward, grad clipping and optimization step\n",
    "#     loss_value.backward()\n",
    "#     # this is not strictly mandatory but it's good practice to keep\n",
    "#     # your gradient norm bounded\n",
    "#     torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "#     optim.step()\n",
    "#     optim.zero_grad()\n",
    "\n",
    "# logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "# pbar.update(tensordict_data.numel())\n",
    "# cum_reward_str = (\n",
    "#     f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "# )\n",
    "# logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "# stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "# logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "# lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if i % 10 == 0:\n",
    "#     # We evaluate the policy once every 10 batches of data.\n",
    "#     # Evaluation is rather simple: execute the policy without exploration\n",
    "#     # (take the expected value of the action distribution) for a given\n",
    "#     # number of steps (1000, which is our ``env`` horizon).\n",
    "#     # The ``rollout`` method of the ``env`` can take a policy as argument:\n",
    "#     # it will then execute this policy at each step.\n",
    "#     with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n",
    "#         # execute a rollout with the trained policy\n",
    "#         eval_rollout = env.rollout(1000, policy_module)\n",
    "#         logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "#         logs[\"eval reward (sum)\"].append(\n",
    "#             eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "#         )\n",
    "#         logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n",
    "#         eval_str = (\n",
    "#             f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "#             f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "#             f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "#         )\n",
    "#         del eval_rollout\n",
    "# pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "916ed45b-aff1-46a0-b2c3-c2b1aebdfe6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4419,  0.0302,  0.1637,  0.3082, -0.0841,  0.0662,  0.0974,\n",
       "          -0.2352,  1.1254,  1.0621,  0.8163,  1.0777,  1.0454,  0.8775,\n",
       "           1.0421,  0.8418]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ud.as_tensor_dict()\n",
    "\n",
    "with T.no_grad():\n",
    "    data = pg_net.loss_module.actor_network(data)\n",
    "    data = pg_net.gae(data)\n",
    "\n",
    "\n",
    "# out = pg_net.loss_module(data)\n",
    "\n",
    "# loss_value = (\n",
    "#     out[\"loss_objective\"]\n",
    "#     + out[\"loss_critic\"]\n",
    "#     + out[\"loss_entropy\"]\n",
    "# )\n",
    "\n",
    "# loss_value\n",
    "loc, scale, action, log_prob = pg_net.loss_module.actor_network(T.rand(1, 1, 32))\n",
    "# pg_net.loss_module.actor_network.dist_sample_keys\n",
    "\n",
    "T.cat([loc,scale], dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
